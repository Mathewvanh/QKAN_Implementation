import unittest
import torch
import numpy as np
from typing import Tuple

from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import transforms

from KAN_w_cumulative_polynomials import FixedKANConfig, FixedKAN, KANNeuron


class TestFixedKAN(unittest.TestCase):
    def setUp(self):
        """Setup basic test configurations"""
        self.config = FixedKANConfig(
            network_shape=[1, 10, 1],  # Single input/output with 10 hidden neurons
            max_degree=7,
            complexity_weight=0.1,
            trainable_coefficients=False,
            # ---- Fix: skip QUBO for hidden to avoid dimension mismatch ----
            skip_qubo_for_hidden=True,
            default_hidden_degree=3
        )
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    @staticmethod
    def target_function(x: torch.Tensor) -> torch.Tensor:
        # Initialize output tensor
        y = torch.zeros_like(x)

        # Create masks
        mask1 = x < 0.5
        mask2 = (x >= 0.5) & (x < 1.5)
        mask3 = x >= 1.5

        # Apply functions to each region
        y[mask1] = torch.sin(20 * torch.pi * x[mask1]) + x[mask1].pow(2)
        y[mask2] = 0.5 * x[mask2] * torch.exp(-x[mask2]) + torch.abs(torch.sin(5 * torch.pi * x[mask2]))
        y[mask3] = torch.log(x[mask3] - 1) / torch.log(torch.tensor(2.0)) - torch.cos(2 * torch.pi * x[mask3])

        # Add noise - using PyTorch's normal distribution
        noise = torch.normal(mean=0.0, std=0.2, size=y.shape, device=y.device)
        y += noise

        return y

    def generate_test_data(self, func: callable, n_samples: int = 1000) -> Tuple[torch.Tensor, torch.Tensor]:
        """Generate test data for a given function"""
        x = torch.linspace(-1, 1, n_samples, device=self.device).reshape(-1, 1)
        y = func(x).reshape(-1, 1)
        return x, y

    def test_simple_function(self):
        """Test fitting a simple polynomial function"""
        def simple_func(x: torch.Tensor) -> torch.Tensor:
            return 0.5 * x**2 - 0.3 * x + 0.1

        # Generate data
        x_data, y_data = self.generate_test_data(simple_func)

        # Create and optimize network
        #  -- override skip_qubo_for_hidden here too if needed --
        kan = FixedKAN(self.config)
        kan.optimize(x_data, y_data)

        # Make predictions
        with torch.no_grad():
            y_pred = kan(x_data)

        # Check MSE
        mse = torch.mean((y_pred - y_data) ** 2)
        print(f"Simple function MSE: {mse.item()}")
        self.assertLess(mse.item(), 0.1)  # Should fit well

        # Verify architecture
        self.assertEqual(len(kan.layers), 2)  # Input->Hidden, Hidden->Output
        self.assertEqual(len(kan.layers[0].neurons), 10)  # 10 hidden neurons

        # Check degrees were assigned
        for layer in kan.layers:
            for neuron in layer.neurons:
                self.assertIsNotNone(neuron.selected_degree)
                self.assertLessEqual(neuron.selected_degree, self.config.max_degree)

    def test_complex_function(self):
        """Test fitting a more complex function"""
        def complex_func(x: torch.Tensor) -> torch.Tensor:
            return torch.sin(2 * np.pi * torch.cos(x**2)) + 0.5 * torch.cos(2 * np.pi * torch.exp(x**2))

        # Generate data
        x_data, y_data = self.generate_test_data(complex_func, n_samples=1000)

        # Create network with more hidden neurons
        # ---- Also skip QUBO for hidden layers to avoid dimension mismatch ----
        config = FixedKANConfig(
            network_shape=[1, 5, 1],
            max_degree=5,
            skip_qubo_for_hidden=True
        )
        kan = FixedKAN(config)

        # Optimize and predict
        kan.optimize(x_data, y_data)
        with torch.no_grad():
            y_pred = kan(x_data)

        # Check MSE
        mse = torch.mean((y_pred - y_data) ** 2)
        print(f"Complex function MSE: {mse.item()}")
        # self.assertLess(mse.item(), 0.1)

    def test_multi_layer_network(self):
        """Test a deeper network architecture"""
        # We'll reuse the 'target_function' from setUp.
        x_data, y_data = self.generate_test_data(self.target_function, n_samples=1000)

        # Create deeper network
        # ---- skip_qubo_for_hidden = True to avoid shape mismatch on hidden layers
        config = FixedKANConfig(
            network_shape=[1, 10, 10, 1],  # Three layers
            max_degree=5,
            skip_qubo_for_hidden=True
        )
        kan = FixedKAN(config)

        # Optimize and predict
        kan.optimize(x_data, y_data)
        with torch.no_grad():
            y_pred = kan(x_data)

        # Check MSE
        mse = torch.mean((y_pred - y_data) ** 2)
        print(f"Complex function MSE: {mse.item()}")
        # self.assertLess(mse.item(), 0.1)
        # (Analysis and visualization code omitted for brevity)

    def test_multivariate_fractal(self):
        """Test fitting a complex multivariate fractal function and verify save/load works."""

        @staticmethod
        def fractal_func(x: torch.Tensor) -> torch.Tensor:
            """Fractal-like 2D test function with multiple features"""
            x_coord = x[:, 0]
            y_coord = x[:, 1]

            # Multiple frequency components
            z = torch.sin(10 * torch.pi * x_coord) * torch.cos(10 * torch.pi * y_coord) + \
                torch.sin(torch.pi * (x_coord**2 + y_coord**2))

            # Non-linear interactions
            z += torch.abs(x_coord - y_coord) + \
                 (torch.sin(5 * x_coord * y_coord) / (0.1 + torch.abs(x_coord + y_coord)))

            # Envelope
            z *= torch.exp(-0.1 * (x_coord**2 + y_coord**2))

            # Add noise
            noise = torch.normal(0, 0.1, z.shape, device=x.device)
            z += noise

            return z.unsqueeze(-1)  # [batch_size, 1]

        n_samples = 500
        x = torch.linspace(-1, 1, n_samples)
        y = torch.linspace(-1, 1, n_samples)
        X, Y = torch.meshgrid(x, y, indexing='ij')
        x_data = torch.stack([X.flatten(), Y.flatten()], dim=1)  # [2500, 2]
        y_data = fractal_func(x_data)                           # [2500, 1]

        # Create network
        # ---- again, skip QUBO for hidden so we avoid out-of-bounds for hidden > 1
        config = FixedKANConfig(
            network_shape=[2, 5, 5, 1],
            max_degree=5,
            skip_qubo_for_hidden=True
        )
        kan = FixedKAN(config)

        kan.optimize(x_data, y_data)
        with torch.no_grad():
            y_pred = kan(x_data)

        # Compute Error
        mse = torch.mean((y_pred - y_data) ** 2)
        rel_error = mse / torch.sum(y_data**2)
        print(f'y_data: {y_data.shape}')
        print(f"[Before training] Fractal function MSE={mse.item():.6f}, Relative MSE={rel_error.item()}")

        print(f'\n---- Training model after QUBO ----')
        # (Optional further training code)

        # Save model
        save_path = "temp_fractal_kan.pth"
        kan.save_model(save_path)
        print(f"Model saved to: {save_path}")

        # Load & Compare
        loaded_kan = FixedKAN.load_model(save_path)
        with torch.no_grad():
            y_pred_loaded = loaded_kan(x_data)
        diff_mse = torch.mean((y_pred_loaded - y_pred)**2)
        print(f"Prediction MSE difference after load: {diff_mse.item()}")

        # (Optional 3D plotting code)

    def test_mnist_dimensionality(self):
        """Test that network can handle MNIST-like high dimensional input"""
        batch_size = 10000
        input_dim = 4000  # 28x28 pixels
        num_classes = 10

        # Create random data
        x_data = torch.randn(batch_size, input_dim)
        y_data = torch.randint(0, num_classes, (batch_size, 1)).float()

        config = FixedKANConfig(
            network_shape=[4000, 32, 16, 10],
            max_degree=3,
            skip_qubo_for_hidden=True  # safe to skip QUBO for these big hidden layers as well
        )
        kan = FixedKAN(config)

        # This should work without dimension errors
        kan.optimize(x_data, y_data)

        # Test forward pass
        with torch.no_grad():
            y_pred = kan(x_data)

        # Check output dimensions
        self.assertEqual(y_pred.shape, (batch_size, num_classes))

    def test_save_load(self):
        """Test saving and loading the model preserves structure and predictions"""
        def simple_func(x: torch.Tensor) -> torch.Tensor:
            return 0.5 * x**2 - 0.3 * x + 0.1

        # Generate data
        x_data, y_data = self.generate_test_data(simple_func)

        # Create and optimize original network
        original_kan = FixedKAN(self.config)
        original_kan.optimize(x_data, y_data)

        # Get original predictions
        with torch.no_grad():
            original_pred = original_kan(x_data)
            original_mse = torch.mean((original_pred - y_data) ** 2)

        # Save the model
        save_path = 'test_kan_save.pt'
        original_kan.save_model(save_path)

        # Load the model
        loaded_kan = FixedKAN.load_model(save_path)

        # Check predictions are identical
        with torch.no_grad():
            loaded_pred = loaded_kan(x_data)
            loaded_mse = torch.mean((loaded_pred - y_data) ** 2)

        self.assertTrue(torch.allclose(original_pred, loaded_pred))
        self.assertEqual(original_mse.item(), loaded_mse.item())

        # Test model structure is preserved
        for orig_layer, loaded_layer in zip(original_kan.layers, loaded_kan.layers):
            self.assertEqual(len(orig_layer.neurons), len(loaded_layer.neurons))

            for orig_neuron, loaded_neuron in zip(orig_layer.neurons, loaded_layer.neurons):
                self.assertEqual(orig_neuron.degree, loaded_neuron.degree)
                self.assertEqual(len(orig_neuron.coefficients), len(loaded_neuron.coefficients))
                for orig_coeff, loaded_coeff in zip(orig_neuron.coefficients, loaded_neuron.coefficients):
                    self.assertTrue(torch.allclose(orig_coeff, loaded_coeff))

        # Clean up
        import os
        os.remove(save_path)


if __name__ == '__main__':
    unittest.main(argv=['first-arg-is-ignored'], exit=False)